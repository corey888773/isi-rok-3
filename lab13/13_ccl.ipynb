{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"hQZtm-vvhmlZ","pycharm":{"name":"#%% md\n"}},"source":["# Indeksacja. Prosta klasyfikacja obiektów.\n","\n","## Cel ćwiczenia\n","- zapoznanie z dwuprzebiegowym algorytmem indeksacji (ang. *connected component labelling*):\n","- zapoznanie z prostą klasyfikacją obiektów z wykorzystaniem współczynników kształtu,\n","- zadanie domowe: zmodyfikowana indeksacja dwuprzebiegowa i indeksacja jednoprzebiegowa.\n","\n","## Indeksacja\n","\n","Większość dotychczas poznanych i wykorzystywanych algorytmów przetwarzania obrazu wykonywała operacje na całym obrazie (lub co najwyżej dokonywany był podział obiekt/obiekty - tło. Np. różne metody segmentacji).\n","Patrząc na poniższny rysunek człowiek widzi dwa kwadraty.\n","\n","![Dwa kwadraty, obraz binarny](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/2objects.png)\n","\n","Dla automatycznego systemu analizy i rozpoznawania obrazów przejście od etapu obiekty/tło do wyróżnienia dwóch kwadratów nie jest takie oczywiste i wymaga zastosowania jakieś formy indeksacji - czyli przypisania każdemu z pikseli uznanych za należące do obiektów (tu białych) jakieś etykiety (w naszym przypadku będą to dwie etykiety, ew. dodatkowa oznaczająca tło).\n","\n","W tej części ćwiczenia zaprezentowany zostanie tzw. dwuprzebiegowy algorytm indeksacji (ang. *two-pass connected component labelling*).\n","Jego nazwa związana jest z koniecznością wykonania dwukrotnego odwiedzenia każdego z pikseli obrazu (dwie pętle/iteracje po obrazie).\n","Istnieją również tzw. jednoprzebiegowe algorytmy indeksacji."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"RffZdb9lhmlf"},"source":["## Opis algorytmu\n","\n","- Na wejściu mamy obraz po binaryzacji - zakładamy, że piksele białe ('1', '255') należą do obiektów, tło jest czarne.\n","\n","- W pierwszej iteracji obraz analizujemy linia po linii, od lewej do prawej, aż do napotkania pierwszego piksela o wartości różnej od '0'.\n","  W tej sytuacji dokonujemy analizy otoczenia piksela:\n","\n","  ![Dwa kwadraty, obraz binarny](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclContext.png)\n","\n","  gdzie: **X**  rozważany piksel, **A,B,C,D** - sąsiedzi z otoczenia.\n","  Uwaga. W przykładzie pokazano tzw. sąsiedztwo 8-elementowe.\n","  Niekiedy stosuje się również wersję 4-elementową (praktycznie bardzo rzadko).\n","  Wtedy analizie podlegałyby by tylko piksele **B** i **D**.\n","\n","- Możliwe są następujące przypadki dla otoczenia __A,B,C,D__:\n","    - (a) wszystkie należą do tła __A,B,C,D == 0__.\n","      Wtedy znaleziony piksel __X__ należy do nowego obiektu - nadajemy mu zatem etykietę __X = L+1__ - przez __L__ rozumiemy poprzednią etykietę.\n","\t  Uwaga. Przed uruchomieniem algorytmu __L__ należy zainicjować wartością  '1'\n","    - (b) jeden lub więcej pikseli ma przypisaną aktualną etykietę **L**.\n","\t  Wtedy rozważanemu pikselowi przypisujemy etykietę **X = L**.\n","    - (c) w otoczeniu występują piksele o różnych etykietach np. *L1* i *L2*.\n","\t  Wtedy przyjmuje się zasadę, że rozważanemu pikselowi *X* przypisuje się mniejszą z wartości *L1* i *L2* (*X = min(L1,L2)*)."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"p-tJCJ5shmlh"},"source":["## Implementacja:\n","\n","- Wczytaj obraz *ccl1.png*, wyświetl go.\n","  Po krótkiej analizie można zauważyć, że występuje na nim 7 odrębnych obiektów.\n","      Ich ``wykrycie'' będzie naszym celem.\n","\n","- Na podstawie opisu algorytmu podanego wcześniej zaimplementuj tzw. pierwszą fazę indeksacji (ang. _first pass_):\n","    - uwaga ogólna - algorytm jest dość prosty i łatwy w implementacji,\n","    - obliczenia powinny odbywać się w pętlach `for` - iteracja po całym obrazku.\n","      Warto pominąć pierwszy wiersz i pierwszą oraz ostatnią kolumnę, aby  wyeliminować problem z brakiem kontekstu (można np. założyć, że pikseli brzegowych nie ma, bo wcześniej wykonane zostało morfologiczne czyszczenie brzegu).\n","    - dla przypomnienia iteracja po macierzy w Python - pętla zewnętrzna - wiersze, pętla wewnętrzna - kolumny – tj. pierwsza współrzędna to wiersz, a druga kolumna,\n","    - rozmiar obrazka można odczytać np. poleceniem: `yy, xx = img.shape`\n","      Przy takim przypisaniu `yy` oznacza liczbę wierszy (wysokość obrazka), a `xx` liczbę kolumn (szerokość obrazka),\n","    - działania podejmujemy tylko w przypadku, gdy aktualnie analizowany piksel ma wartość różną od zera (gdy należy do obiektu),\n","    - najtrudniejszym elementem jest analiza otoczenia piksela i stwierdzenie, z którym z przypadków (a), (b) czy (c) mamy do czynienia.\n","      Problem można rozwiązać jakkolwiek (tylko dobrze), poniżej prezentowana jest jedna z możliwości.\n","    - na początku tworzymy listę pikseli, które stanowią otoczenie piksela X (A, B, C, D) np: ` neighbours = [img[j - 1, i - 1], ... `\n","    - następnie sprawdzamy czy nie występuje przypadek (a) - czyli czy suma sąsiadów nie wynosi 0 -  ` neigh_sum = np.sum(neighbours)`, jeżeli tak to `X = L` oraz `L=L+1`.\n","      Uwaga proszę nie zapomnieć zainicjalizować etykiety `L` wartością 1.\n","    - jeżeli `neigh_sum > 0`, mamy do czynienia z przypadkiem (b) lub (c).\n","      Eliminujemy zerowe elementy z listy `sasiedzi` (`neighbours = filter(lambda x: x != 0, neighbours)`), a następnie znajdujemy minimum i maksimum nowej listy `neighbours` (funkcje `min` i `max` z _numpy_).\n","    - z przypadkiem (b) będziemy mieli do czynienia, gdy *minimum = maksimum*.\n","      Wtedy jako etykietę przypisujemy `X = minimum = maksimum`.\n","      Gdy *minimum != maksimum* mamy przypadek (c).\n","      Wtedy etykieta `X = minimum` - taką przyjmujemy konwencję,\n","    - uwaga 1 - algorytm operuje (czyta punkty z otoczenia i zapisuje kolejne indeksy L) na tym samym obrazie wejściowym,\n","    \\item uwaga 2 - przed implementacją metody obsługi konfliktów przypadki (b) i (c) działają tak samo, ale dla potrzeb dalszych kroków należy je rozróżnić,\n","\n","- Wykonaj indeksację obrazu za pomocą zaimplementowanego algorytmu.\n","  Jeżeli wszystko zostało poprawnie napisane, to wynik powinien wyglądać mniej więcej tak jak na poniższym rysunku. Takie porównanie stanowić będzie pierwszy test poprawności implementacji algorytmu.\n","\n","  ![Wynik pierwszewgo etapu indekscacji](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/ccl1Result.png)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3qjzw2yhmli","outputId":"8026a62d-e30a-411a-bebe-0c59c509d895","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","if not os.path.exists(\"ccl1.png\") :\n","    !wget https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/ccl1.png --no-check-certificate\n","if not os.path.exists(\"shapes.png\") :\n","    !wget https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/shapes.png --no-check-certificate\n","if not os.path.exists(\"shapesReal.png\") :\n","    !wget https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/shapesReal.png --no-check-certificate\n","\n","# Implementacja pierwszego kroku indeksacji\n","\n","def indexing(img):\n","    yy, xx = img.shape\n","    output = np.zeros(img.shape, dtype=np.uint)\n","    L = 1\n","\n","    for i in range(xx):\n","        for j in range(yy):\n","            # remove borders\n","            if i == 0 or j == 0 or i == xx - 1 or j == yy - 1:\n","                continue\n","            # if pixel is black\n","            if img[j, i] == 0:\n","                continue\n","            \n","            somsiady = [\n","                output[j-1, i-1],\n","                output[j-1, i],\n","                output[j-1, i+1],\n","                output[j, i-1]\n","            ]\n","\n","            # a situation\n","            if sum(somsiady) == 0:\n","                output[j, i] = L\n","                L += 1\n","                continue\n","\n","            # b or c\n","            somsiady_bez_zer = list(filter(lambda x: x != 0, somsiady))\n","            min_somsiady, max_somsiady = min(somsiady_bez_zer), max(somsiady_bez_zer)\n","            \n","            # b situation\n","            if min_somsiady == max_somsiady:\n","                output[j, i] = min_somsiady\n","                # print('b', somsiady, somsiady_bez_zer)\n","                continue\n","\n","            # c situation\n","            # print('c', somsiady, somsiady_bez_zer)\n","            output[j, i] = min_somsiady\n","\n","    return output\n","            \n","def print_imgs(img1, img2):\n","    fig, axs = plt.subplots(1, 2, figsize=(15, 15))\n","    axs[0].imshow(img1, 'gray')\n","    axs[1].imshow(img2, 'gray')\n","    plt.show()\n","\n","img = cv2.imread('ccl1.png', cv2.IMREAD_GRAYSCALE)\n","indexed_img = indexing(img)\n","indexed_img = cv2.normalize(indexed_img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","\n","print_imgs(img, indexed_img)\n","            \n","\n","            \n","            \n","            \n","    \n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"sSYQBjjHhmlk"},"source":["## Dyskusja:\n","\n","- W wyniku indeksacji uzyskujemy obraz, na którym wyróżnionych jest wiele obiektów (dokładnie 48) - a faktycznie jest tylko 7. Zastanówmy się z czego to wynika.\n","\n","- Zacznijmy od prostego przypadku przedstawionego na poniższym rysunku:\n","\n","  ![Indeksacja - przykład pierwszy](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx1.png)\n","\n","  Obraz analizowany jest linia po linii.\n","  Zatem pierwszą etykietę dostanie słupek po prawej.\n","  Późnej ten po lewej.\n","  Postępując zgodnie z podanym algorytmem, w pewnym momencie (piksel wyróżniony na obrazku) dojdzie do sytuacji, w której w otoczeniu danego piksela znajdą się dwie różne etykiety (tu: '1' i '2').\n","  Wtedy zgodnie z przyjętą metodologią przypisujemy niższą, tj. '1'.\n","  Problem jaki powstaje, to brak zapamiętania informacji o tym, że wystąpił taki konflikt tj. że powinno nastąpić połączenie etykiet '1' i '2'.\n","  Z tego powodu wynik działania algorytmu nie jest poprawny.\n","\n","-  Przeanalizujmy jeszcze jeden, bardziej złożony przykład z rysunku poniżej:\n","   ![Indeksacja - przykład drugi](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx2.png)\n","\n","   Słupek po prawej dostaje indeks '1', a po lewej '2'.\n","   W czwartej linii występuje piksel, który ma w swoim sąsiedztwie (A,B,C,D) same piksele czarne.\n","   Dlatego dostaje etykietę '3'.\n","   Jednak w następnym kroku okazuje się, że następuje konflikt '3' z '1' (połączenie).\n","   W kolejnym wierszu występuje konflikt '2' i '3'.\n","   Warto zwrócić uwagę, że w tym przypadku trzeba uniknąć sytuacji ''utraty'' informacji o połączeniu '1' z '2' przez '3'.\n","\n","- W literaturze zaproponowano wiele sposobów reprezentacji i rozwiązywania przedstawionych konfliktów:\n","    - 2-krotki,\n","    - n-krotki,\n","    - tablica dwuwymiarowa,\n","    - grafy + przeszukiwanie grafu wgłąb.\n"," W obecnym ćwiczeniu zastosujemy ostatnie podejście, które jest najprostsze do realizacji."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"NKXYR2Nihmll","pycharm":{"name":"#%% md\n"}},"source":["- Opisane konflikty możemy przechowywać w strukturze zbiorów rozłącznych (ang. _union find_).\n","  Jest to zagadnienie znane z przedmiotu ''Algorytmy i struktury danych''.\n","  Poniżej zostanie zaprezentowane krótkie przypomnienie.\n","\n","- Mamy $N$ obiektów. W naszym przypadku to jest $N$ etykiet.\n","  Chcemy przechowywać informację o sytuacji, w której następuje łączenie etykiet tj. interesują nas zbiory obiektów połączonych.\n","  Przykład trzech zbiorów zamieszczono na rysunku:\n","  ![Przykład trzech połączonych zbiorów](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/unionFind1.png)\n","\n","- W ramach rozważanej struktury implementuje się dwie operacje:\n","    - *find* (znajdź) - sprawdzenie czy dwa obiekty należą do tego samego zbioru połączonego,\n","\t- *union* (połącz) - wprowadź połączenie pomiędzy dwoma obiektami.\n","\n","- Z punktu widzenia naszych potrzeb ważna jest funkcja _union_. Przykład dodania połączenia pomiędzy obiektami '2' a'3' pokazano na rysunku:\n","\n","  ![Przykład łączenia obiektów](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/unionFind2.png)\n","\n","- Strukturę oraz obie operacje można zaimplementować na kilka sposobów.\n","  Zainteresowanych odsyłam do literatury przedmiotu.\n","  My zastosujemy podejście _quick-union_.\n","  Nie jest ono specjalnie wydajnie, ale bardzo proste do implementacji.\n","\n","- Nasz graf, w którym wierzchołki oznaczają etykiety, a krawędzie połączenia między etykietami, zapiszemy w tablicy jednowymiarowej `id[]` o rozmiarze $N$ (maksymalna liczba etykiet).\n","  Interpretacja pola w tablicy: `id[i]` jest rodzicem `i`.\n","  Korzeń elementu `i` jest dany jako: `id[id[id[...id[i]...]]]`.\n","\n","- Przeanalizujmy jak to działa na prostym przykładzie.\n","  Mamy dany poetykietowany obraz:\n","\n","    ![Indeksacja - przykład 3](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3.png)\n","\n","  Występuje na nim pięć konfliktów oznaczonych A, B, C, D i E  (uwaga w trakcie działania algorytmu pojawią się właśnie w takiej kolejności).\n","\n","- Sytuację wyjściową opisuje poniższy rysunek:\n","   ![Indeksacja - przykład 3](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union1.png)\n","\n","   Tablica `id` wypełniona jest kolejnymi indeksami.\n","\n","- W algorytmie _quick-union_, aby połączyć elementy $p$ i $q$ należy ustawić $id$ korzenia $q$ pod adres korzenia elementu $p$.\n","Uwaga. Zakładamy, że $p$ < $q$.\n","Można to zapisać jako: `id[root(p)] = root(q)`.\n","Nasze pierwsze połączenie (A) to 4 z 2.\n","Po tej operacji sytuacja będzie wyglądać następująco:\n","\n","    ![Indeksacja - przykład 3_1](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union2.png)\n","\n","W tym przypadku korzenieniem obu elementów są same elementy, zatem operacja jest dość prosta.\n","\n","- Kolejny krok tj. połączenie  2 z 1 -  sytuacja B:\n","\n","    ![Indeksacja - przykład 3_2](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union3.png)\n","\n","Element 1 jest korzeniem.\n","Element 2 już nie, ponieważ `id[2]==4`.\n","Sprawdzamy zatem `id[4]`.\n","Okazuje się, że `id[4] == 4` tj. jest to korzeń.\n","Zatem korzeniem dla elementu 2 jest 4.\n","Dokonujemy stosownej modyfikacji w tablicy `id[1] = 4`.\n","\n","-  Kolejne połączenie to 6 z 3 (C):\n","\n","    ![Indeksacja - przykład 3_3](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union4.png)\n","\n","- Kolejne połączenie to 5 z 1.\n","\n","    ![Indeksacja - przykład 3_4](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union5.png)\n","\n","Szukamy korzenia elementu 1.\n","Znajdujemy, że to jest 4.\n","Zatem łączymy 5 i 4.\n","\n","- Ostatnie połączenie to 6 z 5.\n","\n","   ![Indeksacja - przykład 3_5](https://raw.githubusercontent.com/vision-agh/poc_sw/master/13_CCL/cclEx3_union6.png)\n","\n","-  Ostatni krok to wykorzystanie informacji zawartej w tablicy `id` do wyznaczenia przekodowania LUT (ang. _look-up table_), które pozwoli nam nadać poprawne etykiety wszystkim pikselom.\n","      Zasadniczo sprowadza się on do obliczenia dla każdej możliwej etykiety jej korzenia.\n","      W naszym przypadku zawsze otrzymamy 6, gdyż wszystkie piksele są ze sobą połączone i stanowią jeden obiekt."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"HomFhrpThmlm","pycharm":{"name":"#%% md\n"}},"source":["## Implementacja:\n","\n","- Implementacja jest dużo prostsza i krótsza od powyższego opisu.\n","  Istotne jest tylko dobre zrozumienie algorytmu _union-find_.\n","\n","- Potrzebne będą nam dwie funkcje pomocnicze:\n","\n","    - `root` - obliczanie korzenia zgodnie z podanym opisem (odpowiednia pętla `while`). Funkcja pobiera indeks elementu oraz tablicę, a zwraca indeks korzenia.\n","    - `union` - realizacja operacji unii. Argumenty to indeksy `p` i `q` oraz tablica, a wyniki to zmodyfikowana tablica.\n","\n","- Na początku, przed pierwszym przebiegiem algorytmu indeksacji, tworzymy tablicę `id` i inicjujemy ją wartościami od 1 do $N$.\n","  Dla rozważanego przykładu wystarczy $N = 100$.\n","\n","- Następnie, w przypadku wystąpienia konfliktu (przypadek ()c)), tworzymy unię pomiędzy etykietą mniejszą i większą.\n","\n","- Po pierwszym przebiegu tworzymy, w opisany powyżej sposób, przekodowanie LUT (pętla po `id` i funkcja `root`). Powstaje nam tablica `lut`.\n","\n","- Implementujemy drugi przebieg po obrazie.\n","  Jest on bardzo prosty.\n","  Dla każdego piksela, który nie jest tłem (o etykiecie większej od 0) realizujemy przekodowanie LUT (`lut_table[img[j, i] - 1]`).\n","  Otrzymany w ten sposób obraz wyświetlamy.\n","  Powinniśmy uzyskać poprawne etykietowanie."]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"jvRBYxQzhmln","pycharm":{"name":"#%% md\n"}},"source":["## Indeksacja wbudowana w OpenCV\n","\n","W OpenCV dostępna jest funkcja `connectedComponents`, która pozwala wykonać indeksację.\n","Proszę sprawdzić jej działanie na używanym wcześniej obrazie."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjuZ4UMWhmlo","outputId":"a48a83e1-bb5a-4364-8806-2f9ee90f50c0","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Indeksacja wbudowana w OpenCV\n","\n","#2\n","def root(list, index):\n","    while list[index] != index:\n","        index = list[index]\n","    return index\n","\n","def union(list, p, q):\n","    if p > q:\n","        p, q = q, p\n","\n","    root_p = root(list, p)\n","    root_q = root(list, q)\n","\n","    list[root_q] = root_p\n","\n","def normalize_img_with_list(img, list):\n","    list_set = set(list)\n","    map = {value: index for index, value in enumerate(list_set)} # to lower magnitude of numbers\n","\n","    yy, xx = img.shape\n","    output = np.zeros(img.shape, dtype=np.uint)\n","    for i in range(xx):\n","        for j in range(yy):\n","            if img[j, i] != 0:\n","                output[j, i] = root(list, img[j, i]) # + 1 because we want to start from 1 not 0\n","                output[j, i] = map[output[j, i]] + 1\n","\n","    return output\n","\n","def indexing_with_union(img):\n","    yy, xx = img.shape\n","    output = np.zeros(img.shape, dtype=np.uint)\n","    \n","    #3\n","    L_list = [i for i in range(yy * xx)]\n","    L = 1\n","\n","    for i in range(xx):\n","        for j in range(yy):\n","            # remove borders\n","            if i == 0 or j == 0 or i == xx - 1 or j == yy - 1:\n","                continue\n","            # if pixel is black\n","            if img[j, i] == 0:\n","                continue\n","            \n","            somsiady = [\n","                output[j-1, i-1],\n","                output[j-1, i],\n","                output[j-1, i+1],\n","                output[j, i-1]\n","            ]\n","\n","            # a situation\n","            if sum(somsiady) == 0:\n","                output[j, i] = L\n","                L += 1\n","\n","                continue\n","\n","            # b or c\n","            somsiady_bez_zer = list(filter(lambda x: x != 0, somsiady))\n","            min_somsiady, max_somsiady = min(somsiady_bez_zer), max(somsiady_bez_zer)\n","            \n","            # b situation\n","            if min_somsiady == max_somsiady:\n","                output[j, i] = min_somsiady\n","                continue\n","\n","            # c situation\n","            #4\n","            union(L_list, min_somsiady, max_somsiady)\n","            output[j, i] = min_somsiady\n","            \n","    #6\n","    # this is my helper function\n","    output = normalize_img_with_list(output, L_list[:L])\n","\n","    return output\n","\n","img = cv2.imread('ccl1.png', cv2.IMREAD_GRAYSCALE)\n","indexed_img = indexing_with_union(img)\n","indexed_img = cv2.normalize(indexed_img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","\n","print_imgs(img, indexed_img)\n","            \n","        \n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"PX-mSQU0hmlp"},"source":["## Rozpoznawanie obiektów z wykorzystaniem współczynników kształtu\n","\n","Współczynniki kształtu są pewnymi parametrami liczbowymi opisującymi kształt obiektu.\n","Pozwala to na użycie ich do automatycznego rozpoznawania obiektów.\n","Teoretycznie współczynniki kształtu (dobre) powinny być niezależne od zmiany położenia obiektu, jego orientacji i wielkości.\n","W praktyce okazuje się jednak, że wartości współczynników zmieniają się w pewnym zakresie nawet dla obiektów należących do tych samych klas (te same kształty - przykład obracany kwadrat).\n","Wynika z tego konieczność uwzględnienia pewnej tolerancji wartości współczynników kształtu wykorzystywanych w praktycznych zastosowaniach analizy i rozpoznawania obrazu.\n","\n","# Implementacja dla obrazu sztucznego\n","\n","- Wczytaj obraz _shapes.png_.\n","  Wyświetl go.\n","  Pierwszym etapem zadania będzie wybranie współczynników kształtu i przedziałów ich wartości - takich, które pozwolą na wykrycie konkretnych kształtów.\n","  Do wyboru jest kwadrat lub krzyżyk (koło jest zbyt łatwe).\n","\n","- Obok ''klasycznych'' współczynników kształtu tj. Blair-Bliss, Malinowskiej, Haralick, występują współczynniki będące pochodnymi momentów geometrycznych tzn. momenty Hu [https://en.wikipedia.org/wiki/Image_moment]. Wykorzystamy je w naszym ćwiczeniu.\n","\n","- Pierwszy krok to indeksacja obrazu - poznana funkcja `connectedComponents`.\n","- Następnie w pętli, dla kolejnych obiektów wyznaczamy współczynniki Hu.\n","    - po pierwsze należy ''wybrać'' z obrazu dany obiekt np. `I = (ccl == s)*1` oraz `I = I.astype('uint8')`\n","    - po drugie obliczyć momenty geometryczne - `cv2.moments(I)`\n","    - po trzecie, na podstawie momentów, obliczyć momentu Hu `cv2.HuMoments(moments)`\n","- Mając wyznaczone współczynniki musimy jest przeanalizować. W tym celu dobrze ja jakoś wyświetlić w postaci macierzy - proszę to rozwiązać samodzielnie.\n","\n","- Na podstawie analizy należy wybrać próg (jeden lub więcej) i zrealizować stosowną filtrację tj. usunąć ''niechciane'' kształty.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sUM4QNuhmlq","outputId":"2b1edffc-06e2-4f6d-8ea8-114e7ca875db","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Rozpoznawanie obiektów sztucznych\n","\n","#1\n","shapes = cv2.imread('shapes.png', cv2.IMREAD_GRAYSCALE)\n","plt.imshow(shapes, 'gray')\n","plt.show()\n","\n","#3\n","shapes_indexed = indexing_with_union(shapes)\n","shapes_indexed = cv2.normalize(shapes_indexed, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","plt.imshow(shapes_indexed, 'gray')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#4\n","def analyse_shapes(img):\n","    pixel_values = np.unique(img)\n","    for pixel_value in pixel_values:\n","        if pixel_value == 0:\n","            continue\n","        I = np.where(img == pixel_value, 255, 0)\n","        moments = cv2.moments(I, True)\n","        huMoments = cv2.HuMoments(moments)\n","        print(huMoments)\n","        plt.imshow(I, 'gray')\n","        plt.show()\n","\n","\n","analyse_shapes(shapes_indexed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shapes_map ={\n","    'circle': [1.5e-1, 1.6e-1],\n","    'square': [1.66e-1, 1.67e-1],\n","    'cross': [2.5e-1, 2.6e-1],\n","}\n","\n","def remove_pixels(img, pixel_value):\n","    output = np.where(img == pixel_value, 0, img)\n","    return output\n","\n","def remove_shapes(img, shape, parameters_map):\n","    output = img.copy()\n","    pixel_values = np.unique(img)\n","    for pixel_value in pixel_values:\n","        if pixel_value == 0:\n","            continue\n","        I = np.where(img == pixel_value, 255, 0).astype(np.uint8)\n","        moments = cv2.moments(I, True)\n","        huMoments = cv2.HuMoments(moments)\n","\n","        if huMoments[0] > parameters_map[shape][0] and huMoments[0] < parameters_map[shape][1]:\n","            output = remove_pixels(output, pixel_value)\n","\n","    return output\n","\n","shapes_indexed_without_circles = remove_shapes(shapes_indexed, 'circle', shapes_map)\n","shapes_indexed_without_crosses = remove_shapes(shapes_indexed, 'cross', shapes_map)\n","shapes_indexed_without_squares = remove_shapes(shapes_indexed, 'square', shapes_map)\n","\n","fig, axs = plt.subplots(1, 4, figsize=(15, 15))\n","axs[0].imshow(shapes_indexed, 'gray')\n","axs[1].imshow(shapes_indexed_without_circles, 'gray')\n","axs[2].imshow(shapes_indexed_without_crosses, 'gray')\n","axs[3].imshow(shapes_indexed_without_squares, 'gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"sf2jZ40qhmls"},"source":["# Implementacja dla obrazu rzeczywistego\n","\n","- Podobne ćwiczenie należy zrealizować dla obrazu rzeczywistego _shapesReal.png_.\n","- Uwaga. Wykorzystując poznane metody przetwarzania obrazu doprowadź obraz do postaci binarnej (usuń ew. zakłócenia itp.).\n","- Uwagi:\n","    - należy z obrazu wyeliminować małe grupki pikseli - tak aby nie zostały poindeksowane,\n","    - obiektów powinno być 13,\n","    - może się okazać, że zajdzie potrzeba modyfikacji warunków na współczynniki albo wręcz wprowadzenia dodatkowych współczynników.\n","    - warto zaznaczyć, że opisane podejście tj. ''ręczne'' wyznaczanie przedziałów parametrów stosuje się tylko do bardzo prostych problemów. W rzeczywistych aplikacjach rozpoznawania obrazu stosuje się tzw. uczenie maszynowe i klasyfikatory typu sztuczne sieci neuronowe, algorytm k-NN, czy SVM.\n","\n","- Sposób postępowania jest zasadniczo bardzo podobny do tego użytego wcześniej. Przy czym, ponieważ obiektów jest więcej, warto je sobie ''opisać'' - ułatwia to analizę:\n","    - użyjemy funkcji `connectedComponentsWithStats` - pozwala ona wyliczyć statystyki (pole, prostokąt otaczający oraz centroid). Szczegóły użycia proszę sprawdzić w dokumentacji,\n","    - wykonujemy kopię obrazu poindeksowanego (`ccl_vis = ccl.copy()`),\n","    - w pętli po obiektach wypisujemy numery:\n","      `cv2.putText(ccl_vis,str(label_num),(int(centroids[s,0]),int(centroids[s,1])),cv2.FONT_HERSHEY_SIMPLEX,1,(20,0,0),2)`\n","      Argumenty to odpowiednio: obraz, tekst, jego położenie, czcionka, rozmiar, kolor, grubość).\n","      Zwracam uwagę na kolor, tu dobrze jest dać wartość nieco większą niż maksymalna etykieta.\n","      Inaczej wystąpi problem z czytelnością wizualizacji.\n","\n","- Przeprowadzamy podobną analizę jak wcześniej i dobieramy odpowiednie warunki/progi.\n","- W efekcie powinniśmy uzyskać obraz z krzyżykami, kwadratami lub kółkami."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzK4U1eYhmls","outputId":"3d948d46-8b77-4600-ee5a-063b8c7a9eb9","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# Rozpoznawanie obiektów rzeczywistych\n","\n","real_shapes = cv2.imread('shapesReal.png', cv2.IMREAD_GRAYSCALE)\n","plt.imshow(real_shapes, 'gray')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kernel_size = 3\n","kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n","closed_real_shapes = cv2.morphologyEx(real_shapes, cv2.MORPH_CLOSE, kernel)\n","\n","real_shapes_blurred = cv2.GaussianBlur(closed_real_shapes, (kernel_size, kernel_size), 0)\n","\n","real_shapes_binarized = np.where(real_shapes_blurred > 100, 0, 255).astype(np.uint8)\n","\n","fig, axs = plt.subplots(1, 3, figsize=(15, 15))\n","axs[0].imshow(closed_real_shapes, 'gray')\n","axs[1].imshow(closed_real_shapes, 'gray')\n","axs[2].imshow(real_shapes_binarized, 'gray')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["real_shapes_indexed = indexing_with_union(real_shapes_binarized)\n","real_shapes_indexed = cv2.normalize(real_shapes_indexed, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","plt.imshow(real_shapes_indexed, 'gray')\n","plt.show()\n","\n","print(len(np.unique(real_shapes_indexed)) - 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["analyse_shapes(real_shapes_indexed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shapes_map = {\n","    'triangle': [1.85e-1, 1.87e-1],\n","    'rectangle': [1.65e-1, 1.67e-1],\n","    'circle': [1.59e-1, 1.6e-1],\n","    'cross': [1.86e-1, 2e-1],\n","    'leftturn': [1.76e-1, 1.77e-1],\n","}\n","\n","real_shapes_indexed_without_triangles = remove_shapes(real_shapes_indexed, 'triangle', shapes_map)\n","real_shapes_indexed_without_rectangles = remove_shapes(real_shapes_indexed, 'rectangle', shapes_map)\n","real_shapes_indexed_without_circles = remove_shapes(real_shapes_indexed, 'circle', shapes_map)\n","real_shapes_indexed_without_crosses = remove_shapes(real_shapes_indexed, 'cross', shapes_map)\n","real_shapes_indexed_without_leftturns = remove_shapes(real_shapes_indexed, 'leftturn', shapes_map)\n","\n","fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n","axs[0, 0].imshow(real_shapes_indexed, 'gray')\n","axs[0, 0].set_title('original')\n","\n","axs[0, 1].imshow(real_shapes_indexed_without_triangles, 'gray')\n","axs[0, 1].set_title('without triangles')\n","\n","axs[0, 2].imshow(real_shapes_indexed_without_rectangles, 'gray')\n","axs[0, 2].set_title('without rectangles')\n","\n","axs[1, 0].imshow(real_shapes_indexed_without_circles, 'gray')\n","axs[1, 0].set_title('without circles')\n","\n","axs[1, 1].imshow(real_shapes_indexed_without_crosses, 'gray')\n","axs[1, 1].set_title('without crosses')\n","\n","axs[1, 2].imshow(real_shapes_indexed_without_leftturns, 'gray')\n","axs[1, 2].set_title('without leftturns')\n","\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
